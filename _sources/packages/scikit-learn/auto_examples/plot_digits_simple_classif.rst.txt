

.. _sphx_glr_packages_scikit-learn_auto_examples_plot_digits_simple_classif.py:


Simple visualization and classification of the digits dataset
=============================================================

Plot the first few samples of the digits dataset and a 2D representation
built using PCA, then do a simple classification



.. code-block:: python


    from sklearn.datasets import load_digits
    digits = load_digits()







Plot the data: images of digits
-------------------------------

Each data in a 8x8 image



.. code-block:: python

    from matplotlib import pyplot as plt
    fig = plt.figure(figsize=(6, 6))  # figure size in inches
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

    for i in range(64):
        ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
        ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
        # label the image with the target value
        ax.text(0, 7, str(digits.target[i]))





.. image:: /packages/scikit-learn/auto_examples/images/sphx_glr_plot_digits_simple_classif_001.png
    :align: center




Plot a projection on the 2 first principal axis
------------------------------------------------



.. code-block:: python


    plt.figure()

    from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    proj = pca.fit_transform(digits.data)
    plt.scatter(proj[:, 0], proj[:, 1], c=digits.target, cmap="Paired")
    plt.colorbar()





.. image:: /packages/scikit-learn/auto_examples/images/sphx_glr_plot_digits_simple_classif_002.png
    :align: center




Classify with Gaussian naive Bayes
----------------------------------



.. code-block:: python


    from sklearn.naive_bayes import GaussianNB
    from sklearn.model_selection import train_test_split

    # split the data into training and validation sets
    X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)

    # train the model
    clf = GaussianNB()
    clf.fit(X_train, y_train)

    # use the model to predict the labels of the test data
    predicted = clf.predict(X_test)
    expected = y_test

    # Plot the prediction
    fig = plt.figure(figsize=(6, 6))  # figure size in inches
    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

    # plot the digits: each image is 8x8 pixels
    for i in range(64):
        ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
        ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary,
                  interpolation='nearest')

        # label the image with the target value
        if predicted[i] == expected[i]:
            ax.text(0, 7, str(predicted[i]), color='green')
        else:
            ax.text(0, 7, str(predicted[i]), color='red')





.. image:: /packages/scikit-learn/auto_examples/images/sphx_glr_plot_digits_simple_classif_003.png
    :align: center




Quantify the performance
------------------------

First print the number of correct matches



.. code-block:: python

    matches = (predicted == expected)
    print(matches.sum())




.. rst-class:: sphx-glr-script-out

 Out::

    395


The total number of data points



.. code-block:: python

    print(len(matches))




.. rst-class:: sphx-glr-script-out

 Out::

    450


And now, the ration of correct predictions



.. code-block:: python

    matches.sum() / float(len(matches))







Print the classification report



.. code-block:: python

    from sklearn import metrics
    print(metrics.classification_report(expected, predicted))





.. rst-class:: sphx-glr-script-out

 Out::

    precision    recall  f1-score   support

              0       1.00      1.00      1.00        47
              1       0.74      0.91      0.82        46
              2       0.97      0.81      0.89        48
              3       1.00      0.81      0.89        42
              4       0.96      0.92      0.94        49
              5       0.86      0.95      0.90        40
              6       0.94      0.98      0.96        48
              7       0.79      0.98      0.87        46
              8       0.67      0.73      0.70        45
              9       1.00      0.64      0.78        39

    avg / total       0.89      0.88      0.88       450


Print the confusion matrix



.. code-block:: python

    print(metrics.confusion_matrix(expected, predicted))

    plt.show()





.. rst-class:: sphx-glr-script-out

 Out::

    [[47  0  0  0  0  0  0  0  0  0]
     [ 0 42  0  0  0  0  2  0  2  0]
     [ 0  4 39  0  1  0  1  0  3  0]
     [ 0  1  0 34  0  2  0  1  4  0]
     [ 0  0  0  0 45  0  0  4  0  0]
     [ 0  0  0  0  0 38  0  0  2  0]
     [ 0  0  1  0  0  0 47  0  0  0]
     [ 0  0  0  0  1  0  0 45  0  0]
     [ 0  7  0  0  0  0  0  5 33  0]
     [ 0  3  0  0  0  4  0  2  5 25]]


**Total running time of the script:** ( 0 minutes  12.905 seconds)



.. container:: sphx-glr-footer


  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_digits_simple_classif.py <plot_digits_simple_classif.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_digits_simple_classif.ipynb <plot_digits_simple_classif.ipynb>`

.. rst-class:: sphx-glr-signature

    `Generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
